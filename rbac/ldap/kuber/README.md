
# CFK Deployment with OpenLDAP authentication and RBAC Authorization

Confluent platform contains kafka, zookeeper and kafkaRestClass ( for rolebindings). Goal is to make users and groups in the openLDAP, and then use RBAC to give roles to the particular user and group.

The super users are kafka and rahul

Step 1 : configure OpenLDAP
-
```bash
sudo kubectl apply -f ldap-service.yaml
```
- This will create a OpenLDAP pod in the container. for dashboard port forward and then use it in ApacheDirectoryStudio

```bash
sudo kubectl port-forward openldap-85845bd9b7-gc9f7 10389:389
```
- use get pod command to get the pod name of openldap.
- conduct an LDAP search in the LDAP pod

```bash
sudo kubectl exec -it <ldap-pod-name> -- bash
```
```bash
ldapsearch -x -H ldap://ldap.svc.confluent.cluster.local:389 -b "dc=example,dc=com" -D "cn=admin,dc=example,dc=com" -w secret

```

- This should show all the groups and users created in the OpenLDAP server

Step 2 : Deploy configuration secrets which contains the certificates
-
- we are gonna use the autogenerated certificates so we need to create certificate authority

```bash
openssl genrsa -out ca-key.pem 2048
```
```bash
openssl req -new -key ca-key.pem -x509 \
  -days 1000 \
  -out ca.pem \
  -subj "/CN=TestCA"

```

- this should create a certificate authority(CA) and CA-key with CN as TestCA (issuer). Add it in secrets

```bash
openssl genrsa -out ca-key.pem 2048
```
```bash
kubectl create secret tls ca-pair-sslcerts \
  --cert=ca.pem \
  --key=ca-key.pem -n confluent
```

Step 3 : Provide authentication credentials for kafka
- 
```bash
kubectl create secret generic credential \
  --from-file=plain-users.json=users.json \
  --from-file=plain.txt=users.txt \
  --from-file=basic.txt=creds-control-center-users.txt \
  --from-file=ldap.txt=ldap.txt \
  --namespace confluent
```

Step 4 : Provide authentication credentials for RBAC principal
- 
- once the CA is done, we need to make public key and the tokenkeypair for ldap and kafka connection

```bash
openssl genrsa -out tokenKeypair.pem 2048
```
```bash
openssl rsa -in tokenKeypair.pem -outform PEM -pubout -out publickey.pem
```
```bash
openssl genpkey -algorithm RSA -out privatekey.pem
```
```bash
openssl pkcs8 -topk8 -in private_key.pem -out pkcs8KeypairEncrypted_PBE-SHA1-3DES.pem -v1 PBE-SHA1-3DES
```

- add it in secrets (add txt file so do cat publickey and tokenkeypair and extract the text)

```bash
sudo kubectl create secret generic mds-token \
  --from-file=mdsPublicKey.pem=publickey.txt \
  --from-file=mdsTokenKeyPair.pem=tokenkeypair.txt \
  --namespace confluent
```
- add secrets for kafka, controlcenter and kafka-rest


1. for KAFKA RBAC credential
```bash
sudo kubectl create secret generic mds-client \
  --from-file=bearer.txt=bearer.txt \
  --namespace confluent
```
2. for KafkaRest RBAC credential
```bash
sudo kubectl create secret generic rest-credential \
  --from-file=bearer.txt=bearer.txt \
  --from-file=basic.txt=bearer.txt \
  --namespace confluent
```
3. for Control Center RBAC credential
```bash
sudo kubectl create secret generic c3-mds-client \
  --from-file=bearer.txt=c3-mds-client.txt \
  --namespace confluent
```
Step 5 : Deploy Confluent Platform and check whether all resources are deployed
- 
```bash
sudo kubectl apply -f confluent-secure.yaml \
  --namespace confluent
```
```bash
sudo kubectl get pods
```

Step 6 : Give RoleBindings to the users
- 
- Since we have internal authentication as mtls and external authentication as plain, we can enter the pod and use internal bootstrap-listener as the super user as well.
```bash
sudo kubectl exec -it <kafka-pod> -- bash
```

- create a kafka.properties file 
```bash
cat <<-EOF > /opt/confluentinc/kafka.properties
bootstrap.servers=kafka.confluent.svc.cluster.local:9071
security.protocol=SSL
ssl.truststore.location=/mnt/sslcerts/truststore.jks
ssl.truststore.password=mystorepassword
ssl.endpoint.identification.algorithm=https
ssl.keystore.location=/mnt/sslcerts/keystore.jks
ssl.keystore.password=mystorepassword
ssl.key.password=mystorepassword
EOF
```
- check the cluster ID
```bash
 kafka-cluster cluster-id --bootstrap-server kafka.confluent.svc.cluster.local:9071 --config /opt/confluentinc/kafka.properties
```
- use the cluster id to create a rolebinding.yaml file and provide the id in the specs
- deploy the rolbindings resource as well
```bash
sudo kubectl apply -f rolebindings.yaml
```
- in order to check whether rolbinding is assigned or not we use kafkarest
```bash
sudo kubectl get ConfluentRoleBindings
```

Step 6 : Check whether the rolebinding works or not by producing and consuming the data
-
- login to the kafka pod and create a client.property file based on the roles (Eg: George for the producer)

```bash
cat <<-EOF > /opt/confluentinc/kafka.properties
bootstrap.servers=kafka.confluent.svc.cluster.local:9092
security.protocol=SASL_SSL
ssl.truststore.location=/mnt/sslcerts/truststore.jks
ssl.truststore.password=mystorepassword
ssl.endpoint.identification.algorithm=https
ssl.keystore.location=/mnt/sslcerts/keystore.jks
ssl.keystore.password=mystorepassword
ssl.key.password=mystorepassword
sasl.mechanism=PLAIN
sasl.jaas.config=org.apache.kafka.common.security.plain.PlainLoginModule required \
  username="george" \
  password="georgedeveloper";
EOF
```
- You can do the same for the Java producer code as well, just insert the confugrations in the code for producers / consumers.
